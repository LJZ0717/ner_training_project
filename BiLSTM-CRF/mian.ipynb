{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8986cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    tags.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "# ç”¨æ³•\n",
    "sentences, labels = read_bio_file(r\"C:\\Users\\Administrator\\Desktop\\Project\\bio_dataset_cleaned.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c61d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# åŠ è½½ BERT åˆ†è¯å™¨\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# åˆ›å»ºæ ‡ç­¾æ˜ å°„\n",
    "label_list = sorted(set(label for label_seq in labels for label in label_seq))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def encode_examples(sentences, labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    label_ids = []\n",
    "\n",
    "    for sent, label in zip(sentences, labels):\n",
    "        # åˆ†è¯ï¼ˆæ¯ä¸ªå•è¯ä¸€ä¸ªå…ƒç´ ï¼‰\n",
    "        encoding = tokenizer(\n",
    "            sent,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(label2id[\"O\"])  # ç”¨ 'O' æ›¿æ¢åŸæ¥çš„ -100\n",
    "  # å¿½ç•¥å¡«å……ä½\n",
    "            elif word_idx != prev_word_idx:\n",
    "                aligned_labels.append(label2id[label[word_idx]])  # ç¬¬ä¸€ä¸ªå­è¯ï¼šä¿ç•™æ ‡ç­¾\n",
    "            else:\n",
    "                aligned_labels.append(label2id[\"O\"])  # ç”¨ 'O' æ›¿æ¢åŸæ¥çš„ -100\n",
    "  # åç»­å­è¯ï¼šå¿½ç•¥\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        input_ids.append(encoding['input_ids'][0])\n",
    "        attention_masks.append(encoding['attention_mask'][0])\n",
    "        label_ids.append(torch.tensor(aligned_labels))\n",
    "\n",
    "    return input_ids, attention_masks, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1715e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks, label_ids = encode_examples(sentences, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268638f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.label_ids[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6da522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# æ„å»ºæ•°æ®é›†å¯¹è±¡\n",
    "dataset = NERDataset(input_ids, attention_masks, label_ids)\n",
    "\n",
    "# æ„å»º DataLoaderï¼ˆå¯è®¾ç½® batch_sizeï¼‰\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cebd2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BERT_CRF(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERT_CRF, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        # æ‚¨ä½¿ç”¨çš„æ˜¯é»˜è®¤çš„ batch_first=False\n",
    "        self.crf = CRF(num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        emissions = self.classifier(sequence_output)  # Shape: [batch, seq_len, num_labels]\n",
    "\n",
    "        # ç»Ÿä¸€å°†ç»´åº¦è½¬æ¢ä¸º (seq_len, batch, ...) ä»¥é€‚é…CRFå±‚\n",
    "        emissions = emissions.permute(1, 0, 2)  # Shape: [seq_len, batch, num_labels]\n",
    "        mask = attention_mask.permute(1, 0).bool() # Shape: [seq_len, batch]\n",
    "\n",
    "        if labels is not None:\n",
    "            # è®¡ç®—æŸå¤±æ—¶ï¼Œlabelsä¹Ÿéœ€è¦è½¬æ¢ä¸º (seq_len, batch)\n",
    "            labels = labels.permute(1, 0)\n",
    "            log_likelihood = self.crf(emissions, labels, mask=mask)\n",
    "            loss = -log_likelihood.mean()\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6408f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_CRF(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       "  (crf): CRF(num_tags=15)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# âœ… è¿™é‡Œæ”¹ä¸ºä½¿ç”¨ label2id\n",
    "num_labels = len(label2id)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT_CRF('bert-base-uncased', num_labels)\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3944332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e59183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799f37fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [04:18<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 128.2122\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [04:25<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 95.9285\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [04:26<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 75.2487\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [04:18<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 56.6552\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [04:18<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 44.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    avg_loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d62929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "  \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. ç§»é™¤ breakï¼Œéå†æ•´ä¸ª dataloader\n",
    "        for batch in dataloader:\n",
    "            # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # æ¨¡å‹åœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œå¦‚æœä¸ä¼ å…¥ labelsï¼Œåˆ™ä¼šè¿›å…¥è§£ç ï¼ˆdecodeï¼‰é€»è¾‘\n",
    "            predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # 2. æ”¶é›†æ¨¡å‹çš„é¢„æµ‹ç»“æœ\n",
    "            # model çš„è¾“å‡º (predictions) å·²ç»æ˜¯å¤„ç†å¥½çš„ list of lists\n",
    "            all_preds.extend(predictions)\n",
    "\n",
    "            # 3. å¤„ç†å¹¶æ”¶é›†çœŸå®æ ‡ç­¾\n",
    "            # éœ€è¦æ ¹æ® attention_mask ç§»é™¤ padding éƒ¨åˆ†çš„æ ‡ç­¾\n",
    "            for i in range(labels.shape[0]):\n",
    "                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„çœŸå®é•¿åº¦ï¼ˆépaddingéƒ¨åˆ†ï¼‰\n",
    "                true_length = attention_mask[i].sum().item()\n",
    "                # æˆªå–çœŸå®æ ‡ç­¾å¹¶è½¬æ¢ä¸º list\n",
    "                true_labels = labels[i][:true_length].tolist()\n",
    "                all_labels.append(true_labels)\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç¡®ä¿ä» seqeval.metrics å¯¼å…¥äº†è¿™ä¸‰ä¸ªå‡½æ•°\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "def evaluate(preds, trues, id2label):\n",
    "    # å°† ID åºåˆ—è½¬æ¢æˆæ ‡ç­¾åºåˆ— (è¿™éƒ¨åˆ†ä¸å˜)\n",
    "    preds_label = [[id2label[idx] for idx in seq] for seq in preds]\n",
    "    trues_label = [[id2label[idx] for idx in seq] for seq in trues]\n",
    "\n",
    "    # æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š (è¿™éƒ¨åˆ†ä¸å˜)\n",
    "    print(\"åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(trues_label, preds_label, mode='strict', scheme=IOB2))\n",
    "\n",
    "    p = precision_score(trues_label, preds_label)\n",
    "    r = recall_score(trues_label, preds_label)\n",
    "    f1 = f1_score(trues_label, preds_label)\n",
    "\n",
    "    print(\"\\n--- æ€»ä½“æ€§èƒ½æŒ‡æ ‡ ---\")\n",
    "    print(f\"Overall Precision: {p:.4f}\")\n",
    "    print(f\"Overall Recall:    {r:.4f}\")\n",
    "    print(f\"Overall F1-Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e7dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   AGE_DEATH       0.77      0.72      0.74        32\n",
      "AGE_FOLLOWUP       0.74      0.65      0.69        74\n",
      "   AGE_ONSET       0.82      0.74      0.78       111\n",
      "        GENE       0.89      0.91      0.90       245\n",
      "GENE_VARIANT       0.86      0.93      0.89       392\n",
      "    HPO_TERM       0.96      0.97      0.97      2457\n",
      "     PATIENT       0.83      0.90      0.86       299\n",
      "\n",
      "   micro avg       0.92      0.94      0.93      3610\n",
      "   macro avg       0.84      0.83      0.83      3610\n",
      "weighted avg       0.92      0.94      0.93      3610\n",
      "\n",
      "\n",
      "--- æ€»ä½“æ€§èƒ½æŒ‡æ ‡ ---\n",
      "Overall Precision: 0.9249\n",
      "Overall Recall:    0.9516\n",
      "Overall F1-Score:  0.9380\n"
     ]
    }
   ],
   "source": [
    "# å‡è®¾ä½ ç”¨çš„æ˜¯è®­ç»ƒé›†\n",
    "preds, trues = predict(model, train_loader, device)\n",
    "evaluate(preds, trues, id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
