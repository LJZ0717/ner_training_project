{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8986cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    tags.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "# 用法\n",
    "sentences, labels = read_bio_file(r\"C:\\Users\\Administrator\\Desktop\\Project\\bio_dataset_cleaned.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c61d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# 加载 BERT 分词器\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 创建标签映射\n",
    "label_list = sorted(set(label for label_seq in labels for label in label_seq))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def encode_examples(sentences, labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    label_ids = []\n",
    "\n",
    "    for sent, label in zip(sentences, labels):\n",
    "        # 分词（每个单词一个元素）\n",
    "        encoding = tokenizer(\n",
    "            sent,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(label2id[\"O\"])  # 用 'O' 替换原来的 -100\n",
    "  # 忽略填充位\n",
    "            elif word_idx != prev_word_idx:\n",
    "                aligned_labels.append(label2id[label[word_idx]])  # 第一个子词：保留标签\n",
    "            else:\n",
    "                aligned_labels.append(label2id[\"O\"])  # 用 'O' 替换原来的 -100\n",
    "  # 后续子词：忽略\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        input_ids.append(encoding['input_ids'][0])\n",
    "        attention_masks.append(encoding['attention_mask'][0])\n",
    "        label_ids.append(torch.tensor(aligned_labels))\n",
    "\n",
    "    return input_ids, attention_masks, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1715e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks, label_ids = encode_examples(sentences, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268638f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.label_ids[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6da522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 构建数据集对象\n",
    "dataset = NERDataset(input_ids, attention_masks, label_ids)\n",
    "\n",
    "# 构建 DataLoader（可设置 batch_size）\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BERT_CRF(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERT_CRF, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        # 您使用的是默认的 batch_first=False\n",
    "        self.crf = CRF(num_labels, batch_first=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        emissions = self.classifier(sequence_output)  # Shape: [batch, seq_len, num_labels]\n",
    "\n",
    "        # 统一将维度转换为 (seq_len, batch, ...) 以适配CRF层\n",
    "        emissions = emissions.permute(1, 0, 2)  # Shape: [seq_len, batch, num_labels]\n",
    "        mask = attention_mask.permute(1, 0).bool() # Shape: [seq_len, batch]\n",
    "\n",
    "        if labels is not None:\n",
    "            # 计算损失时，labels也需要转换为 (seq_len, batch)\n",
    "            labels = labels.permute(1, 0)\n",
    "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6408f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_CRF(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ✅ 这里改为使用 label2id\n",
    "num_labels = len(label2id)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT_CRF('bert-base-uncased', num_labels)\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3944332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 训练轮数\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e59183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.mean().item()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799f37fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/95 [00:00<?, ?it/s]f:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████| 95/95 [04:21<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 4.5374\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [04:36<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.3601\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [04:33<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    avg_loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76d62929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # ✅ 打印模型输出与输入信息\n",
    "            print(f\"predictions type: {type(predictions)}\")\n",
    "            print(f\"len(predictions): {len(predictions)}\")\n",
    "            print(f\"type(predictions[0]): {type(predictions[0])}\")\n",
    "            print(f\"predictions[0] 示例: {predictions[0]}\")\n",
    "            print(f\"labels.shape: {labels.shape}\")\n",
    "            print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "            break  # 只看第一个 batch\n",
    "\n",
    "    return [], []\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ee39bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))  # 看有没有数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9babeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "def evaluate(preds, trues, id2label):\n",
    "    # 将 ID 序列转换成标签序列\n",
    "    preds_label = [[id2label[idx] for idx in seq] for seq in preds]\n",
    "    trues_label = [[id2label[idx] for idx in seq] for seq in trues]\n",
    "\n",
    "    print(\"📊 分类报告:\")\n",
    "    print(classification_report(trues_label, preds_label, mode='strict', scheme=IOB2))\n",
    "    print(f\"F1-score: {f1_score(trues_label, preds_label):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e7dff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 假设你用的是训练集\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m preds, trues \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m evaluate(preds, trues, id2label)\n",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred_seq, label_seq, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels, attention_mask):\n\u001b[0;32m     15\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mbool()\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mBERT_CRF.forward\u001b[1;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emission, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(emissions, mask):\n\u001b[0;32m     33\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 34\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviterbi_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43memission\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\torchcrf\\__init__.py:63\u001b[0m, in \u001b[0;36mCRF.viterbi_decode\u001b[1;34m(self, h, mask)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mviterbi_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, h: FloatTensor, mask: BoolTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    decode labels using viterbi algorithm\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    :param h: hidden matrix (batch_size, seq_len, num_labels)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    :return: labels of each sequence in mini batch\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     batch_size, seq_len, _ \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# prepare the sequence lengths in each sequence\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     seq_lens \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# 假设你用的是训练集\n",
    "preds, trues = predict(model, train_loader, device)\n",
    "evaluate(preds, trues, id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fde082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['input_ids', 'attention_mask', 'label_ids', 'tag2id', 'id2tag'])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"ner_data.pkl\", \"rb\") as f:\n",
    "    ner_data = pickle.load(f)\n",
    "print(type(ner_data))\n",
    "print(ner_data.keys())  # 如果是 dict\n",
    "print(len(ner_data))    # 如果是 list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
