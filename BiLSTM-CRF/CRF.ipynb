{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e12583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def read_bio_file(file_path):\n",
    "    \"\"\"从BIO格式的文件中读取句子和标签。\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    tags.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae49865",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_bio_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- 请修改为您自己的文件路径 ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAdministrator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbio_dataset_cleaned.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m sentences, labels \u001b[38;5;241m=\u001b[39m \u001b[43mread_bio_file\u001b[49m(file_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 加载BERT分词器\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_bio_file' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 请修改为您自己的文件路径 ---\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\Project\\bio_dataset_cleaned.txt\"\n",
    "sentences, labels = read_bio_file(file_path)\n",
    "\n",
    "# 加载BERT分词器\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 创建标签到ID的映射\n",
    "label_list = sorted(set(label for label_seq in labels for label in label_seq))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def encode_examples(sentences, labels, max_length=128):\n",
    "    \"\"\"将文本和标签编码为模型输入格式。\"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    label_ids = []\n",
    "\n",
    "    for sent, label_seq in zip(sentences, labels):\n",
    "        encoding = tokenizer(\n",
    "            sent,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(label2id[\"O\"])\n",
    "            elif word_idx != prev_word_idx:\n",
    "                aligned_labels.append(label2id[label_seq[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(label2id[\"O\"])\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        input_ids.append(encoding['input_ids'][0])\n",
    "        attention_masks.append(encoding['attention_mask'][0])\n",
    "        label_ids.append(torch.tensor(aligned_labels))\n",
    "\n",
    "    return input_ids, attention_masks, label_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
