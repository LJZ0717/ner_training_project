{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e12583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def read_bio_file(file_path):\n",
    "    \"\"\"从BIO格式的文件中读取句子和标签。\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    tags.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae49865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 请修改为您自己的文件路径 ---\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\Project\\combined_dataset.txt\"\n",
    "sentences, labels = read_bio_file(file_path)\n",
    "\n",
    "# 加载BERT分词器\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 创建标签到ID的映射\n",
    "label_list = sorted(set(label for label_seq in labels for label in label_seq))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def encode_examples(sentences, labels, max_length=128):\n",
    "    \"\"\"将文本和标签编码为模型输入格式。\"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    label_ids = []\n",
    "\n",
    "    for sent, label_seq in zip(sentences, labels):\n",
    "        encoding = tokenizer(\n",
    "            sent,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(label2id[\"O\"])\n",
    "            elif word_idx != prev_word_idx:\n",
    "                aligned_labels.append(label2id[label_seq[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(label2id[\"O\"])\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        input_ids.append(encoding['input_ids'][0])\n",
    "        attention_masks.append(encoding['attention_mask'][0])\n",
    "        label_ids.append(torch.tensor(aligned_labels))\n",
    "\n",
    "    return input_ids, attention_masks, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c8b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集批次数: 296\n",
      "验证集批次数: 74\n"
     ]
    }
   ],
   "source": [
    "# 按照80/20的比例划分训练集和验证集\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 为两组数据分别进行编码\n",
    "train_input_ids, train_masks, train_labels = encode_examples(train_texts, train_tags)\n",
    "val_input_ids, val_masks, val_labels = encode_examples(val_texts, val_tags)\n",
    "\n",
    "# 定义Dataset类\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 创建Dataset对象\n",
    "train_dataset = NERDataset(train_input_ids, train_masks, train_labels)\n",
    "val_dataset = NERDataset(val_input_ids, val_masks, val_labels)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"训练集批次数: {len(train_loader)}\")\n",
    "print(f\"验证集批次数: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfad4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class BERT_CRF(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERT_CRF, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True) # 设置 batch_first=True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            # 计算损失，mask需要是bool类型\n",
    "            loss = -self.crf(emissions, labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            # 解码预测，mask需要是bool类型\n",
    "            predictions = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ec7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    \"\"\"训练一个epoch。\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss.backward() # 直接反向传播，因为loss已经是单个值\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() # 直接取item\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    \"\"\"进行预测。\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_preds.extend(predictions)\n",
    "\n",
    "            for i in range(labels.shape[0]):\n",
    "                actual_len = attention_mask[i].sum().item()\n",
    "                all_labels.append(labels[i][:actual_len].tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def evaluate(preds, trues, id2label):\n",
    "    \"\"\"计算并打印评估指标。\"\"\"\n",
    "    preds_label = [[id2label[idx] for idx in seq] for seq in preds]\n",
    "    trues_label = [[id2label[idx] for idx in seq] for seq in trues]\n",
    "\n",
    "    print(\"📊 分类报告:\")\n",
    "    print(classification_report(trues_label, preds_label, mode='strict', scheme=IOB2))\n",
    "\n",
    "    p = precision_score(trues_label, preds_label)\n",
    "    r = recall_score(trues_label, preds_label)\n",
    "    f1 = f1_score(trues_label, preds_label)\n",
    "\n",
    "    print(\"\\n--- 总体性能指标 ---\")\n",
    "    print(f\"Overall Precision: {p:.4f}\")\n",
    "    print(f\"Overall Recall:    {r:.4f}\")\n",
    "    print(f\"Overall F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079445cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/296 [00:00<?, ?it/s]f:\\Anaconda\\envs\\ner_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████| 296/296 [14:16<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 17.5083\n",
      "Evaluating on validation set...\n",
      "Validation F1-Score: 0.7865\n",
      "✅ New best F1-Score! Saving model to best_model_on_validation.pth\n",
      "------------------------------\n",
      "--- Epoch 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 296/296 [14:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 6.0278\n",
      "Evaluating on validation set...\n",
      "Validation F1-Score: 0.8293\n",
      "✅ New best F1-Score! Saving model to best_model_on_validation.pth\n",
      "------------------------------\n",
      "--- Epoch 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 296/296 [14:17<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.8373\n",
      "Evaluating on validation set...\n",
      "Validation F1-Score: 0.8537\n",
      "✅ New best F1-Score! Saving model to best_model_on_validation.pth\n",
      "------------------------------\n",
      "--- Epoch 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 296/296 [14:30<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 2.5748\n",
      "Evaluating on validation set...\n",
      "Validation F1-Score: 0.8550\n",
      "✅ New best F1-Score! Saving model to best_model_on_validation.pth\n",
      "------------------------------\n",
      "--- Epoch 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 296/296 [14:39<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.6995\n",
      "Evaluating on validation set...\n",
      "Validation F1-Score: 0.8619\n",
      "✅ New best F1-Score! Saving model to best_model_on_validation.pth\n",
      "------------------------------\n",
      "Training finished.\n",
      "Best F1-Score on Validation Set: 0.8619\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和优化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT_CRF('bert-base-uncased', num_labels).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 5 # 训练轮数\n",
    "\n",
    "# --- 训练与验证循环 ---\n",
    "best_val_f1 = 0.0\n",
    "model_save_path = \"best_model_on_validation.pth\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "    \n",
    "    avg_loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Evaluating on validation set...\")\n",
    "    preds, trues = predict(model, val_loader, device)\n",
    "    \n",
    "    preds_label = [[id2label[idx] for idx in seq] for seq in preds]\n",
    "    trues_label = [[id2label[idx] for idx in seq] for seq in trues]\n",
    "    val_f1 = f1_score(trues_label, preds_label)\n",
    "    \n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        print(f\"✅ New best F1-Score! Saving model to {model_save_path}\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best F1-Score on Validation Set: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b207b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Report for the Best Model on Validation Set ---\n",
      "📊 分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   AGE_DEATH       0.50      0.22      0.31         9\n",
      "AGE_FOLLOWUP       0.60      0.60      0.60        10\n",
      "   AGE_ONSET       0.42      0.56      0.48        18\n",
      "        GENE       0.79      0.76      0.78        75\n",
      "GENE_VARIANT       0.74      0.81      0.78       102\n",
      "    HPO_TERM       0.89      0.88      0.89      1592\n",
      "     PATIENT       0.66      0.76      0.71        80\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1886\n",
      "   macro avg       0.66      0.66      0.65      1886\n",
      "weighted avg       0.86      0.86      0.86      1886\n",
      "\n",
      "\n",
      "--- 总体性能指标 ---\n",
      "Overall Precision: 0.8554\n",
      "Overall Recall:    0.8685\n",
      "Overall F1-Score:  0.8619\n"
     ]
    }
   ],
   "source": [
    "# 加载性能最好的模型\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "print(\"\\n--- Final Report for the Best Model on Validation Set ---\")\n",
    "val_preds, val_trues = predict(model, val_loader, device)\n",
    "evaluate(val_preds, val_trues, id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
