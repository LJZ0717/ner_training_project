{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4951964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "from transformers import BertModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# ========== Reproducibility ==========\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ========== Device ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========== Data ==========\n",
    "def load_data(file_path: str) -> Tuple[List[List[Tuple[str, str]]], dict, dict, List[str]]:\n",
    "    \"\"\"Read BIO format data: each line has a 'word tag', and sentences are separated by blank lines\"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    labels = set()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                word = ' '.join(parts[:-1])\n",
    "                tag = parts[-1]\n",
    "                sentence.append((word, tag))\n",
    "                if tag != 'O' and '-' in tag:\n",
    "                    labels.add(tag.split('-')[1])\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    unique_tags = sorted({tag for sent in sentences for _, tag in sent} | {'O'})\n",
    "    tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "    return sentences, tag2idx, idx2tag, sorted(list(labels))\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tag2idx, tokenizer, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.o_id = self.tag2idx['O']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        words = [w for w, _ in sentence]\n",
    "        tags = [t for _, t in sentence]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    \n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        labels = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(self.o_id)  \n",
    "            elif word_idx != prev_word_idx:\n",
    "                labels.append(self.tag2idx[tags[word_idx]])\n",
    "            else:\n",
    "                # Subsequent subwords of the same word: If it is B-, it continues to I-\n",
    "                if tags[word_idx].startswith('B-'):\n",
    "                    ent = tags[word_idx].split('-')[1]\n",
    "                    labels.append(self.tag2idx.get(f'I-{ent}', self.tag2idx[tags[word_idx]]))\n",
    "                else:\n",
    "                    labels.append(self.tag2idx[tags[word_idx]])\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# ========== Model ==========\n",
    "class BERT_CRF_NER(nn.Module):\n",
    "    def __init__(self, bert_model: str, num_tags: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emissions = self.classifier(self.dropout(outputs.last_hidden_state))\n",
    "        mask = attention_mask.bool()\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=mask)\n",
    "\n",
    "\n",
    "# ========== Train / Eval ==========\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, idx2tag):\n",
    "    \"\"\"Return: micro-P/R/F1, ACC (entity-only), report_text\"\"\"\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "            preds = model(input_ids, attention_mask)  # list[list[int]]\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                mask_i = attention_mask[i].cpu().numpy().astype(bool)\n",
    "                valid_len = int(mask_i.sum())\n",
    "\n",
    "                pred_tags = [idx2tag[p] for p in preds[i][:valid_len]]\n",
    "                true_tags = [idx2tag[l] for l in labels[i][:valid_len]]\n",
    "\n",
    "                m = min(len(pred_tags), len(true_tags))\n",
    "                pred_labels.append(pred_tags[:m])\n",
    "                true_labels.append(true_tags[:m])\n",
    "\n",
    "\n",
    "    report_text = classification_report(true_labels, pred_labels, digits=2, zero_division=0)\n",
    "\n",
    "    # micro 平均\n",
    "    micro_p = precision_score(true_labels, pred_labels, average='micro', zero_division=0)\n",
    "    micro_r = recall_score(true_labels, pred_labels, average='micro', zero_division=0)\n",
    "    micro_f1 = f1_score(true_labels, pred_labels, average='micro', zero_division=0)\n",
    "\n",
    "\n",
    "    correct = total = 0\n",
    "    for t_seq, p_seq in zip(true_labels, pred_labels):\n",
    "        for t, p in zip(t_seq, p_seq):\n",
    "            if t != 'O':\n",
    "                total += 1\n",
    "                if t == p:\n",
    "                    correct += 1\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    return micro_p, micro_r, micro_f1, acc, report_text\n",
    "\n",
    "\n",
    "# ========== Main ==========\n",
    "def main():\n",
    "    # ---- Config ----\n",
    "    DATA_PATH = \"bio_dataset_cleaned.txt\"  \n",
    "    BIOBERT_MODEL = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    LR = 1e-4\n",
    "    DROPOUT = 0.1\n",
    "    SAVE_PATH = \"best_BioBERT_model.pt\"  \n",
    "\n",
    "    # 1) Load data\n",
    "    sentences, tag2idx, idx2tag, entity_types = load_data(DATA_PATH)\n",
    "    print(f\"Loaded {len(sentences)} sentences\")\n",
    "    print(f\"Entity types: {entity_types}\")\n",
    "    print(f\"Num tags: {len(tag2idx)}\")\n",
    "\n",
    "    # 2) Split\n",
    "    train_sents, val_sents = train_test_split(sentences, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "\n",
    "    # 3) Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BIOBERT_MODEL)\n",
    "\n",
    "    # 4) Datasets / Loaders\n",
    "    train_ds = NERDataset(train_sents, tag2idx, tokenizer, MAX_LEN)\n",
    "    val_ds = NERDataset(val_sents, tag2idx, tokenizer, MAX_LEN)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # 5) Model & Optimizer & Scheduler\n",
    "    model = BERT_CRF_NER(BIOBERT_MODEL, len(tag2idx), DROPOUT).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=max(1, int(0.1 * total_steps)),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 6) Train loop with best model tracking (by micro-F1)\n",
    "    best_f1 = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    print(\"Starting training with BioBERT+CRF ...\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        p, r, f1, acc, _ = evaluate(model, val_loader, device, idx2tag)\n",
    "\n",
    "        # === 你要的输出格式 ===\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Test Metrics: Precision={p:.4f}, Recall={r:.4f}, F1={f1:.4f}, ACC={acc:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "    print(f\"Best epoch = {best_epoch}, micro-F1 = {best_f1:.4f}\")\n",
    "    print(f\"Best model saved as {SAVE_PATH}\")\n",
    "\n",
    "    # 7) Final report with full table + overall indicators (micro)\n",
    "    print(\"Final Evaluation on Test Set:\")\n",
    "    p, r, f1, acc, report_text = evaluate(model, val_loader, device, idx2tag)\n",
    "    print(report_text)\n",
    "    print(\"\\n--- Overall performance indicators ---\")\n",
    "    print(f\"Overall Precision: {p:.4f}\")\n",
    "    print(f\"Overall Recall:    {r:.4f}\")\n",
    "    print(f\"Overall F1-Score:  {f1:.4f}\")\n",
    "    print(f\"Entity-only Accuracy (ACC): {acc:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*Gold+Silver*-\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from torchcrf import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "# ========== Configure==========\n",
    "FOCUS_SINGLE_ENTITY = True\n",
    "TARGET_ENTITY = \"HPO_TERM\"\n",
    "\n",
    "GOLD_BIO   = \"bio_dataset_cleaned.txt\"\n",
    "SILVER_BIO = \"chatgpt_integrated_bio_no_punctuation.txt\"  \n",
    "\n",
    "BERT_MODEL    = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "MAX_LEN       = 256\n",
    "BATCH_SIZE    = 32\n",
    "EPOCHS        = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT_PROB  = 0.1\n",
    "EARLY_STOP_PATIENCE = 5  \n",
    "\n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== data load ==========\n",
    "def load_sentences_only(file_path):\n",
    "    \"\"\"Read BIO file -> List[List[(word, tag)]], and the original tag set appears\"\"\"\n",
    "    sentences, sentence = [], []\n",
    "    tags_seen = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                word = ' '.join(parts[:-1])\n",
    "                tag = parts[-1]\n",
    "                sentence.append((word, tag))\n",
    "                tags_seen.add(tag)\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tags_seen\n",
    "\n",
    "def remap_to_single_entity(sentences, target=\"HPO_TERM\"):\n",
    "    \"\"\"Set all B-*/I-* labels except target to O; keep O and B/I-target.\"\"\"\n",
    "    out = []\n",
    "    for sent in sentences:\n",
    "        new_sent = []\n",
    "        for w, t in sent:\n",
    "            if t == \"O\":\n",
    "                new_sent.append((w, \"O\"))\n",
    "            else:\n",
    "                if \"-\" in t:\n",
    "                    bi, ent = t.split(\"-\", 1)\n",
    "                    if ent == target and bi in (\"B\", \"I\"):\n",
    "                        new_sent.append((w, f\"{bi}-{target}\"))\n",
    "                    else:\n",
    "                        new_sent.append((w, \"O\"))\n",
    "                else:\n",
    "                    new_sent.append((w, \"O\"))\n",
    "        out.append(new_sent)\n",
    "    return out\n",
    "\n",
    "# ========== Dataset ==========\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tag2idx, tokenizer, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        words = [w for w, _ in sentence]\n",
    "        tags  = [t for _, t in sentence]\n",
    "        enc = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Word ID mapping (batch_index must be specified in batch mode)\n",
    "        word_ids = enc.word_ids(batch_index=0)\n",
    "\n",
    "        labels = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # [CLS]/[SEP]/PAD\n",
    "                labels.append(self.tag2idx['O'])   # Use 'O' as the id, rather than hard-coding 0\n",
    "            elif word_idx != prev_word_idx:        \n",
    "                labels.append(self.tag2idx[tags[word_idx]])\n",
    "            else:                                  \n",
    "                if tags[word_idx].startswith('B-'):\n",
    "                    labels.append(self.tag2idx['I-' + tags[word_idx].split('-')[1]])\n",
    "                else:\n",
    "                    labels.append(self.tag2idx[tags[word_idx]])\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ========== Model ==========\n",
    "class BERT_CRF_NER(nn.Module):\n",
    "    def __init__(self, bert_model, num_tags, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emissions = self.classifier(self.dropout(outputs.last_hidden_state))\n",
    "        mask = attention_mask.bool()\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=mask)\n",
    "\n",
    "# ========== Train ==========\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(dataloader))\n",
    "\n",
    "# ========== Evalu ==========\n",
    "def _collect_true_pred(model, dataloader, device, idx2tag):\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            preds = model(input_ids, attention_mask)  # List[List[int]]\n",
    "            for i in range(len(preds)):\n",
    "                mask = attention_mask[i].cpu().numpy().astype(bool)\n",
    "                valid_len = int(mask.sum())\n",
    "                pred_tags = [idx2tag[p] for p in preds[i][:valid_len]]\n",
    "                true_tags = [idx2tag[int(l)] for l in labels[i][:valid_len]]\n",
    "                L = min(len(true_tags), len(pred_tags))\n",
    "                true_labels.append(true_tags[:L])\n",
    "                pred_labels.append(pred_tags[:L])\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "def evaluate(model, dataloader, device, idx2tag):\n",
    "    true_labels, pred_labels = _collect_true_pred(model, dataloader, device, idx2tag)\n",
    "\n",
    "\n",
    "    report_text = classification_report(true_labels, pred_labels, digits=2, zero_division=0)\n",
    "\n",
    "    # Overall\n",
    "    precision = precision_score(true_labels, pred_labels, average='micro', zero_division=0)\n",
    "    recall    = recall_score(true_labels, pred_labels, average='micro', zero_division=0)\n",
    "    f1        = f1_score(true_labels, pred_labels, average='micro', zero_division=0)\n",
    "\n",
    "    correct = total = 0\n",
    "    for t_seq, p_seq in zip(true_labels, pred_labels):\n",
    "        for t, p in zip(t_seq, p_seq):\n",
    "            if t != 'O':\n",
    "                total += 1\n",
    "                if t == p:\n",
    "                    correct += 1\n",
    "    acc = correct / total if total else 0.0\n",
    "    return precision, recall, f1, acc, report_text\n",
    "\n",
    "# ========== main ==========\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    gold_sents, gold_tags = load_sentences_only(GOLD_BIO)\n",
    "    silver_sents, silver_tags = load_sentences_only(SILVER_BIO)\n",
    "\n",
    "    print(f\"[GOLD] sentences:   {len(gold_sents)}\")\n",
    "    print(f\"[SILVER] sentences: {len(silver_sents)}\")\n",
    "\n",
    "    # 2) Focus only on HPO_TERM: map all non-target entities to O\n",
    "    if FOCUS_SINGLE_ENTITY:\n",
    "        gold_sents   = remap_to_single_entity(gold_sents, TARGET_ENTITY)\n",
    "        silver_sents = remap_to_single_entity(silver_sents, TARGET_ENTITY)\n",
    "\n",
    "    # 3) GOLD 80/20 (20% as validation set)\n",
    "    train_gold, val_gold = train_test_split(gold_sents, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # 4) Training set = gold80% + all silver; validation set = gold20%\n",
    "    train_sents = train_gold + silver_sents\n",
    "    val_sents   = val_gold\n",
    "    print(f\"[TRAIN] {len(train_sents)} (= gold80% + silver100%)\")\n",
    "    print(f\"[VAL]   {len(val_sents)}   (= gold20%)\")\n",
    "\n",
    "\n",
    "    if FOCUS_SINGLE_ENTITY:\n",
    "        unique_tags = [\"O\", f\"B-{TARGET_ENTITY}\", f\"I-{TARGET_ENTITY}\"]\n",
    "    else:\n",
    "        all_tags = (gold_tags | silver_tags | {\"O\"})\n",
    "        unique_tags = sorted(all_tags)\n",
    "\n",
    "    tag2idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "    idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "    print(\"Tags:\", unique_tags)\n",
    "\n",
    "    # 6) DataLoader\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)\n",
    "    train_dataset = NERDataset(train_sents, tag2idx, tokenizer, MAX_LEN)\n",
    "    val_dataset   = NERDataset(val_sents,   tag2idx, tokenizer, MAX_LEN)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "    model = BERT_CRF_NER(BERT_MODEL, len(tag2idx), DROPOUT_PROB).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    " \n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "\n",
    "    # 8) Training + Validation Monitoring + Best Save + Early Stopping\n",
    "    best_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    no_improve = 0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        precision, recall, f1, acc, report_text = evaluate(model, val_loader, device, idx2tag)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Metrics: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}, ACC={acc:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        \n",
    "        with open(f\"reports/epoch_{epoch:02d}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Classification Report (per-class) - Epoch {epoch}\\n\")\n",
    "            f.write(report_text)\n",
    "            f.write(\"\\n\\n--- Overall (micro) ---\\n\")\n",
    "            f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "            f.write(f\"Recall:    {recall:.4f}\\n\")\n",
    "            f.write(f\"F1:        {f1:.4f}\\n\")\n",
    "            f.write(f\"ACC(entity-only): {acc:.4f}\\n\")\n",
    "\n",
    "        # — Best save & early stop count —\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epoch = epoch\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_BioBERT_model_gold_silver.pt\")\n",
    "            torch.save({\"tag2idx\": tag2idx, \"idx2tag\": idx2tag}, \"label_mapping_gold_silver.pt\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if EARLY_STOP_PATIENCE > 0 and no_improve >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement for {EARLY_STOP_PATIENCE} epochs).\")\n",
    "                break\n",
    "\n",
    "    print(f\"Best epoch = {best_epoch}, micro-F1 = {best_f1:.4f}\")\n",
    "    print(\"Best model saved as best_BioBERT_model.pt\")\n",
    "\n",
    "    # 9) \n",
    "    print(\"Final Evaluation on Validation (20% GOLD):\")\n",
    "    precision, recall, f1, acc, report_text = evaluate(model, val_loader, device, idx2tag)\n",
    "    print(report_text)\n",
    "    print(\"\\n--- Overall performance indicators ---\")\n",
    "    print(f\"Overall Precision: {precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {recall:.4f}\")\n",
    "    print(f\"Overall F1-Score:  {f1:.4f}\")\n",
    "    print(f\"Entity-only Accuracy (ACC): {acc:.4f}\")\n",
    "\n",
    "    with open(\"reports/final_val_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "        f.write(\"\\n\\n--- Overall performance indicators ---\\n\")\n",
    "        f.write(f\"Overall Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Overall Recall:    {recall:.4f}\\n\")\n",
    "        f.write(f\"Overall F1-Score:  {f1:.4f}\\n\")\n",
    "        f.write(f\"Entity-only Accuracy (ACC): {acc:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
